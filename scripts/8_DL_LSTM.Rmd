---
title: "DEEP LEARNING, LSTM"
output:
  word_document: default
  html_notebook: default
  html_document: default
---

# Downloading libraries
```{r}
#devtools::install_github("rstudio/keras")
# then install Tensorflow backend as follows:
library(keras)
#install_keras()
library(tensorflow)
```

# Downloading data
```{r}
#Set Working Directory
#setwd('D:/ML')
#OR Choose your Directory in 'Files' and click on 'More' -> 'Set as Working Directory' #Download file to the table.
f <- read.csv('iPhone.csv', header = TRUE, encoding = 'UNICOD')
plot(f)
```

## Transforming data 
```{r}
#to stationarity
df = diff(f$search, differences = 1)
df <- as.data.frame(df)

#train set size
N = nrow(df)
n = round(N *0.7, digits = 0)

#scaling training set
ymax <- max(df$df[1:n])
ymin <- min(df$df[1:n])
df$y <- 0
df$y[1:n] <- (df$df[1:n] - ymin ) / (ymax - ymin)

#scaling testing set
#ymax <- max(df$df[(n+1):N])
#ymin <- min(df$df[(n+1):N])
df$y[(n+1):N] <- (df$df[(n+1):N] - ymin ) / (ymax - ymin)
df$x <- dplyr::lag(df$y)
head (df)
```

## Splitting the scaled dataset into the TRAIN set and TEST set
```{r}
train = df[2:n, ]
test  = df[(n+1):N,  ]
x_train <- train$x
y_train <- train$y
x_test <- test$x
y_test <- test$y
```

# Defining the model
```{r}
#We set the argument stateful= TRUE so that the internal states obtained after processing a batch of samples are reused as initial states for the samples of the next batch. Since the network is stateful, we have to provide the input batch in 3-dimensional array of the form [samples, timesteps, features] from the current [samples, features], where:
#Samples: Number of observations in each batch, also known as the batch size.
#Timesteps: Separate time steps for a given observations. In this example the timesteps = 1
#Features: For a univariate case, like in this example, the features = 1
#The batch size must be a common factor of sizes of both the training and testing samples. 1 is always a sure bet.
#==========================================================================
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1            # must be a common factor of both the train and test samples
units = 1                 # can adjust this, in model tuninig phase
#==========================================================================
model <- keras_model_sequential() 
model%>%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
  layer_dense(units = 1)
```

## Compiling the model
```{r}
#Here we have specified the mean_squared_error as the loss function, Adaptive Monument Estimation (ADAM) as the optimization algorithm and learning rate and learning rate decay over each update. Finaly, we have used the accuracy as the metric to assess the model performance.
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),  
  metrics = c('accuracy')
)
```

## Model summary
```{r}
summary(model)
```

## Fitting the model
```{r results='hide'}
#We set the argument shuffle = FALSE to avoid shuffling the training set and maintain the dependencies between xi and xi+t. LSTM also requires resetting of the network state after each epoch. To achive this we run a loop over epochs where within each epoch we fit the model and reset the states via the argument reset_states().
Epochs = 50   
for(i in 1:Epochs ){
  model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %>% reset_states()
}
```

## Making predictions
```{r}
L = length(y_test)
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     predictions[i] <- yhat
}

mse_lstm<-sum((y_test-predictions)^2)/L
mse_lstm
library(ggplot2)
plot(y_test, predictions)
```

## Visualising
```{r}
library(ggplot2)
ggplot() +
#  geom_point(aes(f$date[2:n], y_train[2:n]),colour = 'red') +
  geom_point(aes(f$date[(n+1):N], y_test),colour = 'dark green', size = 3) +
  geom_point(aes(f$date[(n+1):N], predictions),colour = 'blue', size = 1) +
  ggtitle('iPhone in time') +
  xlab('date') +
  ylab('iPhone')
```

## Inverting
```{r}
# invert scaling
ys = predictions * (ymax - ymin) + ymin
plot(df$df[(n+1):N],ys)

# invert differencing
fd = numeric(N-n-1)
fd[1] <- f$search[n+1] + ys[1]

for(i in 2:(N-n-1)){
     fd[i] = fd[i-1] + ys[i]
}

library(ggplot2)
plot(f$search[(n+2):N], fd)
```

## Visualising
```{r}
library(ggplot2)
ggplot() +
  geom_point(aes(f$date[2:(n+1)], f$search[2:(n+1)]),colour = 'red') +
  geom_point(aes(f$date[(n+2):N], f$search[(n+2):N]),colour = 'dark green', size = 3) +
  geom_point(aes(f$date[(n+2):N], fd),colour = 'blue', size = 1) +
  ggtitle('iPhone in time') +
  xlab('date') +
  ylab('iPhone')
```