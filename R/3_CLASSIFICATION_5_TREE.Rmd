---
title: "Classification Tree"
output:
  word_document: default
  html_notebook: default
  html_document: default
---

# Download the data
```{r}
set.seed(123)
#setwd('D:/ML')
f <- read.csv('bank.csv', header = TRUE, encoding = 'UNICOD')
f <- f[-1]
```

## Splitting the dataset into the TRAIN set and TEST set
```{r}
set.seed(123)
library(caTools)
split = sample.split(f$delays, SplitRatio = 2/3)
f_train = subset(f, split == TRUE)
f_test = subset(f, split == FALSE)
```

# Fitting 
```{r}
# install.packages('rpart')
library(rpart)
class_dt = rpart(delays ~ ., data = f_train)
```

## Predicting
```{r}
y <- predict(class_dt, f_test[-9], type = 'class')
```

## Confusion Matrix
```{r}
cm = table(f_test[, 'delays'], y)
print(cm)
```

# Plotting the tree
```{r}
plot(class_dt)
text(class_dt)
```

# Less features
```{r}
ff <- f[,c('age','income','delays')]
```

## Features Scaling
```{r}
sc <- scale(ff[-3])
ff$age <- sc[,c('age')]
ff$income <- sc[,c('income')]
```

## Splitting the scaled dataset into the TRAIN set and TEST set
```{r}
set.seed(123)
library(caTools)
split = sample.split(ff$delays, SplitRatio = 2/3)
f_train = subset(ff, split == TRUE)
f_test = subset(ff, split == FALSE)
```

# Fitting 
```{r}
class_ct = rpart(delays ~ ., data = f_train)
```

## Predicting
```{r}
y <- predict(class_ct, f_test[, c('age','income')], type = 'class')
```

## Confusion Matrix
```{r}
cm = table(f_test[, 'delays'], y)
print(cm)
```

# Visualising the Test set results
```{r}
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = f_test[,c('age','income','delays')]
X1 = seq(min(set['age']) - 1, max(set['age']) + 1, by = 0.01)
X2 = seq(min(set['income']) - 1, max(set['income']) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('age', 'income')
y_grid = predict(class_ct, grid_set, type = 'class')
plot(set[, -3],
     main = 'Classification Tree',
     xlab = 'Age', ylab = 'Income',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 'YES', 'tomato', 'springgreen3'))
points(set, pch = 21, bg = ifelse(set[, 3] == 'YES', 'red3', 'green4'))
```

# Fitting Random Forest Classification to the Training set
```{r}
# install.packages('randomForest')
library(randomForest)
set.seed(123)
class_rf = randomForest(delays ~ ., data = f_train, ntree = 50)
```

## Predicting
```{r}
y <- predict(class_rf, f_test[, c('age','income')], type = 'class')
```

## Confusion Matrix
```{r}
cm = table(f_test[, 'delays'], y)
print(cm)
```

# Visualising the Test set results
```{r}
set = f_test[,c('age','income','delays')]
X1 = seq(min(set['age']) - 1, max(set['age']) + 1, by = 0.01)
X2 = seq(min(set['income']) - 1, max(set['income']) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('age', 'income')
y_grid = predict(class_rf, grid_set, type = 'class')
plot(set[, -3],
     main = 'Random Forest',
     xlab = 'Age', ylab = 'Income',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 'YES', 'tomato', 'springgreen3'))
points(set, pch = 21, bg = ifelse(set[, 3] == 'YES', 'red3', 'green4'))
```

# Encoding the categorical variables as factors
```{r}
f$region = as.numeric(factor(f$region, levels = c('TOWN', 'INNER_CITY', 'sUBURBAN', 'RURAL'),labels = c(1, 2, 3, 4)))
f$sex = as.numeric(factor(f$sex,levels = c('FEMALE', 'MALE'),
                                labels = c(1, 2)))
f$married = as.numeric(f$married)
f$car = as.numeric(f$car)
f$mortgage = as.numeric(f$mortgage)
f$delays = as.numeric(f$delays)-1
```

## Delete N/A
```{r}
f <- tidyr::drop_na(f)
cat('there are',nrow(f),'rows in the f')
```

## Splitting the coded dataset into the TRAIN set and TEST set
```{r}
set.seed(123)
library(caTools)
split = sample.split(f$delays, SplitRatio = 2/3)
f_train = subset(f, split == TRUE)
f_test = subset(f, split == FALSE)
```

# Fitting XGBoost to the Training set
```{r}
# install.packages('xgboost')
library(xgboost)
class_xboost = xgboost(data = as.matrix(f_train[-9]), label = f_train$delays, nrounds = 10)
```

## Predicting the Test set results
```{r}
y_pred = predict(class_xboost, newdata = as.matrix(f_test[-9]))
y_pred = (y_pred >= 0.5)
```

## Making the Confusion Matrix
```{r}
cm = table(f_test[, 9], y_pred)
print(cm)
```

# Applying k-Fold Cross Validation
```{r}
# install.packages('caret')
library(caret)
folds = createFolds(f_train$delays, k = 10)
cv = lapply(folds, function(x) {
  train_fold = f_train[-x, ]
  test_fold = f_train[x, ]
  class_xboost = xgboost(data = as.matrix(f_train[-9]), label = f_train$delays, nrounds = 10)
  y_pred = predict(class_xboost, newdata = as.matrix(test_fold[-9]))
  y_pred = (y_pred >= 0.5)
  cm = table(test_fold[, 9], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
```