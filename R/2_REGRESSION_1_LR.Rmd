---
title: "LINEAR REGRESSION"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document: default
---

# Download the data
```{r}
#Source files are here
#setwd('D:/ML')

##Features scaling is included in the packages we will work with

#Download the files
f_train <- read.csv2('flats_train.csv', header = TRUE, encoding = 'UNICOD')
f_train <- f_train[,-1]
f_test <- read.csv2('flats_test.csv', header = TRUE, encoding = 'UNICOD')
f_test <- f_test[,-1]
```

# Simple Linear Regression (one factor - m2)

## Fitting Simple Linear Regression to the Training set
```{r}
model_sr <- lm(price ~ m2, f_train)
summary(model_sr)
```

## Predicting
```{r}
p_sr <- predict(model_sr, f_test)

r2_sr <- 1-sum((f_train$price - predict(model_sr, f_train))^2)/sum((f_train$price - mean(f_train$price))^2)
R2_sr <- cor(f_train$price, fitted(model_sr))^2 #simplier ex.

train_mse_sr <- sum((f_train$price-predict(model_sr, f_train))^2)/length(f_train$price)
test_mse_sr <- sum((f_test$price-p_sr)^2)/length(p_sr)

r2_sr
R2_sr
train_mse_sr
test_mse_sr
```

## Visualising
```{r}
library(ggplot2)
ggplot() +
  geom_point(aes(f_train$m2, f_train$price),colour = 'red') +
  geom_point(aes(f_test$m2, f_test$price),colour = 'dark green') +
  geom_line(aes(f_test$m2, p_sr),colour = 'blue') +
  ggtitle('Price vs m2') +
  xlab('m2') +
  ylab('price')
```

# Multiple Linear Regression (many factors)

## All factors
```{r}
model_mr <- lm(data = f_train, price ~ .) 
summary(model_mr)  
```

## Optimized model
```{r}
#as p-value, Pr(>|t|) of variable "type" is higher than significance level (5%), let's exclude this variable from the model
model_opt <- lm(data = f_train, price ~ rooms + location + condition + m2) 
summary(model_opt)  
```

## Prediction
```{r}
p_mr <- predict(model_opt, f_test)

train_mse_opt <- sum((f_train$price-predict(model_opt, f_train))^2)/length(f_train$price)
test_mse_opt <- sum((f_test$price-p_mr)^2)/length(p_mr)

train_mse_opt
test_mse_opt
```

## Visualising
```{r}
ggplot() +
  geom_point(aes(f_train$m2, f_train$price),colour = 'red') +
  geom_point(aes(f_test$m2, f_test$price),colour = 'dark green') +
  geom_line(aes(f_test$m2, p_mr),colour = 'blue') +
  ggtitle('Price vs m2') +
  xlab('m2') +
  ylab('price')
```

# Polynomial Linear Regression (one factor - m2)

## Features extending 
```{r}
f_train_poly <- f_train[,c('price', 'm2')]
f_test_poly <- f_test[,c('price', 'm2')]
f_train_poly$m22 <- f_train_poly$m2^2
f_train_poly$m23 <- f_train_poly$m2^3
f_test_poly$m22 <- f_test_poly$m2^2
f_test_poly$m23 <- f_test_poly$m2^3
```

## 3 powers
```{r}
model_pr <- lm(data = f_train_poly, price ~ m22 + m23) 
summary(model_pr)  
```

## Predicting
```{r}
p_pr <- predict(model_pr, f_test_poly)

train_mse_poly <- sum((f_train_poly$price-predict(model_pr, f_train_poly))^2)/length(f_train_poly$price)
test_mse_poly <- sum((f_test_poly$price-p_pr)^2)/length(p_pr)

train_mse_poly
test_mse_poly
```

## Visualising
```{r}
ggplot() +
  geom_point(aes(f_train_poly$m2, f_train_poly$price),colour = 'red') +
  geom_point(aes(f_test_poly$m2, f_test_poly$price),colour = 'dark green') +
  geom_line(aes(f_test_poly$m2, p_pr),colour = 'blue') +
  ggtitle('Price vs m2') +
  xlab('m2') +
  ylab('price')
```

# Saving results
```{r}
fit <- data.frame(p_sr, p_mr, p_pr)
write.csv2(fit, file = "flats_fit.csv")
```